{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xFibvPLFzlzP"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-pinecone --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install openai --quiet\n",
        "!pip install pinecone-client --quiet\n",
        "!pip install langchain_openai --quiet\n",
        "!pip install langchain-community --quiet\n",
        "!pip install langchain_upstage --quiet\n",
        "!pip install langchain_core --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_NPqj9fpEQk"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_text_splitters --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtseXMXT0JTf"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_upstage import UpstageEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_upstage import ChatUpstage\n",
        "from openai import OpenAI\n",
        "import pinecone\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtzdYTMl3n9i"
      },
      "outputs": [],
      "source": [
        "UPSTAGE_API_KEY = \"your-api-key\"\n",
        "PINECONE_API_KEY = \"4cf70d26-24f1-4438-8216-2e2b193835e3\"\n",
        "INDEX_NAME = \"greeniche\"\n",
        "PINECONE_ENV = 'gcp-starter'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlDX3vSKVnaf"
      },
      "source": [
        "# 복귀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suhueivH7vWT"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"다음 지시에 따라 질문에 대한 답변을 작성하세요.\n",
        "\n",
        "### 지시:\n",
        "주어진 문서를 바탕으로 질문에 답하세요. 문서의 정보만을 사용하고, 문서에 없는 정보는 사용하지 마세요.\n",
        "\n",
        "### 문서:\n",
        "{context}\n",
        "\n",
        "### 질문:\n",
        "{question}\n",
        "\n",
        "### 답변:\n",
        "\"\"\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bZgoKwID3P6"
      },
      "outputs": [],
      "source": [
        "embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large-query\",\n",
        "                               upstage_api_key=UPSTAGE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SMx-CRWZXEQ"
      },
      "outputs": [],
      "source": [
        "db = PineconeVectorStore(\n",
        "    pinecone_api_key=PINECONE_API_KEY,\n",
        "    index_name=INDEX_NAME,\n",
        "    embedding=embeddings,\n",
        "    text_key='chunk'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oavfkngZeYo"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\": 3, \"score_threshold\": 0.5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd4-fUfqZg90"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    api_key=UPSTAGE_API_KEY,\n",
        "    model_name='solar-pro',\n",
        "    temperature=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgtYaBKGZik7"
      },
      "outputs": [],
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=ChatUpstage(api_key=UPSTAGE_API_KEY, model=\"solar-pro\", max_tokens = 2000),\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    return_source_documents=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMF9JTxlZkYm",
        "outputId": "b7d9fc31-eb8e-4dbd-e0f8-877b80d9b78f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ESRS 정보: 엔디터가 기후변화 대응에 관한 정책, 특히 엔디터의 혁신 확산과 관련된 정책을 설명해야 합니다. ESRS 또한 기후변화에 대한 행동 및 할당된 자원에 대한 설명을 요구합니다.\n",
            "\n",
            "GRI 정보: 조직은 기후변화에 대한 리스크와 기회, 특히 기후 변화에 따른 재무적 영향에 대한 설명, 이러한 리스크와 기회에 대한 설명, 영향에 대한 설명, 대응 조치 전 재무적 영향, 이러한 리스크와 기회를 관리하는 방법, 관리 관련 비용에 대한 설명을 제공해야 합니다.\n",
            "\n",
            "요약하면, ESRS 및 GRI 모두 조직이 기후변화 대응, 특히 기후 변화 대응을 위한 기술 혁신의 확산과 관련된 정책, 행동, 리스크와 기회, 재무 영향에 대한 명확한 설명을 제공해야 합니다.\n"
          ]
        }
      ],
      "source": [
        "query = \"기후변화 대응 위 기술 혁신 확산과 관련된 ESRS, GRI 인덱스는 뭔지 짧게 요약해줘\"\n",
        "result = qa_chain.invoke({\"query\": query})['result']\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo"
      ],
      "metadata": {
        "id": "XE670hYK6J5L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKZeer-94GZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27cc85fc-868f-4637-dfa5-36d718f5529e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVuSIx89sdOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72de52f2-d50f-4208-8443-62aa12b28729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting documentparser.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile documentparser.py\n",
        "import requests\n",
        "import streamlit as st\n",
        "\n",
        "UPSTAGE_API_KEY = \"up_ZL8Fz6IcyZjyREGqzoV0Q4IOSVdho\"\n",
        "\n",
        "def parse_document(uploaded_file):\n",
        "    url = \"https://api.upstage.ai/v1/document-ai/document-parse\"\n",
        "    headers = {\"Authorization\": f\"Bearer {UPSTAGE_API_KEY}\"}\n",
        "\n",
        "    files = {\"document\": uploaded_file.getvalue()}\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, files=files)\n",
        "        response.raise_for_status()  # Raises an HTTPError for bad responses\n",
        "\n",
        "        data = response.json()\n",
        "        if 'content' in data and 'text' in data['content']:\n",
        "            return data['content']['text']\n",
        "        else:\n",
        "            st.error(\"API response does not contain expected 'content.text' field\")\n",
        "            st.json(data)  # Display the full response for debugging\n",
        "            return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        st.error(f\"Error parsing document: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from documentparser import parse_document\n",
        "from ragmodel import initialize_rag, ask_question\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "st.title(\"ESG converting chatbot\")\n",
        "st.write(\"Upload a document and ask questions about it!\")\n",
        "\n",
        "# 사이드바에 제목 추가\n",
        "st.sidebar.title(\"컨텍스트 정보\")\n",
        "\n",
        "# 추가 정보나 컨트롤 추가\n",
        "with st.sidebar:\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"필터\")\n",
        "    selected_option = st.selectbox(\"정보 유형 선택\", [\"모두\", \"ESRS\", \"GRI\"])\n",
        "\n",
        "# 세션 상태 초기화\n",
        "if 'parsed_text' not in st.session_state:\n",
        "    st.session_state.parsed_text = None\n",
        "if 'qa_chain' not in st.session_state:\n",
        "    st.session_state.qa_chain = None\n",
        "\n",
        "# 메모리 상태 초기화\n",
        "if 'memory' not in st.session_state:\n",
        "    st.session_state.memory = ConversationBufferMemory()\n",
        "\n",
        "# 메모리 초기화 버튼\n",
        "if st.sidebar.button('메모리 초기화'):\n",
        "    st.session_state.memory = ConversationBufferMemory()\n",
        "    st.success('메모리가 초기화되었습니다.')\n",
        "\n",
        "# 현재 메모리 상태 표시\n",
        "current_buffer = st.session_state.memory.buffer\n",
        "st.sidebar.write('현재 메모리 상태:', current_buffer)\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload a document (.txt, .md, .pdf)\", type=(\"txt\", \"md\", \"pdf\"))\n",
        "\n",
        "if uploaded_file and st.session_state.parsed_text is None:\n",
        "    with st.spinner(\"문서 분석 중 ...\"):\n",
        "        st.session_state.parsed_text = parse_document(uploaded_file)\n",
        "    st.success(\"Document uploaded and parsed successfully!\")\n",
        "\n",
        "    # RAG 모델 초기화 (문서가 파싱된 후 한 번만 실행)\n",
        "    st.session_state.qa_chain = initialize_rag(st.session_state.parsed_text)\n",
        "\n",
        "if st.session_state.parsed_text is not None:\n",
        "    question = st.text_area(\"질문을 입력하세요:\", placeholder=\"궁금하신 ESRS나 GRI 지표에 대해 질문하세요!\")\n",
        "\n",
        "    if question:\n",
        "        with st.spinner(\"Generating answer...\"):\n",
        "            response = ask_question(st.session_state.qa_chain, question)\n",
        "\n",
        "        st.markdown(\"### Answer:\")\n",
        "        st.markdown(response)\n",
        "\n",
        "        # 메모리에 대화 추가\n",
        "        st.session_state.memory.save_context({\"input\": question}, {\"output\": response})\n",
        "\n",
        "        if \"messages\" not in st.session_state:\n",
        "            st.session_state.messages = []\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "if \"messages\" in st.session_state:\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])"
      ],
      "metadata": {
        "id": "bcN9eOP-1m0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a92e1bd0-3d57-420e-ad64-08d7cc2eaaa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJGCQXOun6wK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ba5f0d-0a73-4aea-b0bf-2ec709fca9fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ragmodel.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ragmodel.py\n",
        "import streamlit as st\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_upstage import UpstageEmbeddings, ChatUpstage\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "UPSTAGE_API_KEY = \"up_ZL8Fz6IcyZjyREGqzoV0Q4IOSVdho\"\n",
        "PINECONE_API_KEY = \"4cf70d26-24f1-4438-8216-2e2b193835e3\"\n",
        "INDEX_NAME = \"upstage\"\n",
        "PINECONE_ENV = 'gcp-starter'\n",
        "@st.cache_resource\n",
        "def initialize_rag(parsed_text):\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"다음 지시에 따라 질문에 대한 답변을 작성하세요.\n",
        "\n",
        "    ### 지시:\n",
        "    주어진 문서를 바탕으로 질문에 답하세요. 문서의 정보만을 사용하고, 문서에 없는 정보는 사용하지 마세요.\n",
        "\n",
        "    ### 문서:\n",
        "    {context}\n",
        "\n",
        "    ### 질문:\n",
        "    {question}\n",
        "\n",
        "    ### 답변:\n",
        "    \"\"\",\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large-query\",\n",
        "                                   upstage_api_key=UPSTAGE_API_KEY)\n",
        "\n",
        "    # 텍스트 분할\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=150, chunk_overlap=0)\n",
        "    texts = text_splitter.split_text(parsed_text)\n",
        "\n",
        "    # 문서 생성\n",
        "    documents = [Document(page_content=t) for t in texts]\n",
        "\n",
        "    # Pinecone에 문서 저장\n",
        "    db = PineconeVectorStore(\n",
        "        pinecone_api_key=PINECONE_API_KEY,\n",
        "        index_name=INDEX_NAME,\n",
        "        embedding=embeddings,\n",
        "        text_key='chunk'\n",
        "    )\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=ChatUpstage(api_key=UPSTAGE_API_KEY, model=\"solar-pro\", max_tokens=1000),\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 1}),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt}\n",
        ")\n",
        "\n",
        "\n",
        "    return qa_chain\n",
        "\n",
        "\n",
        "\n",
        "def ask_question(qa_chain, question):\n",
        "    return qa_chain({\"query\": question})[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FaVTx0Z56DC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8798939-b747-49ca-ccc2-9cc89c2ce4f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "up to date, audited 23 packages in 518ms\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va8_Aj7Z9Y9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331bbebf-bcc8-4363-f203-6bda62989cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password/Endpoint IP: 34.80.215.241\n"
          ]
        }
      ],
      "source": [
        "import urllib\n",
        "print(\"Password/Endpoint IP:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpWQiMfP5kAY"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rm-6M5P9Wnw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1516c731-81ff-4f10-9124-607d51af7362"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "your url is: https://tough-sheep-prove.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H7gWFDWNq5hq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}