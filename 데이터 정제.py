# -*- coding: utf-8 -*-
"""데이터 정제

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m7h3OdFw53FD2LRIwlq9K0n1kzhlWUfQ
"""

!pip install datasets peft bitsandbytes streamlit pyngrok konlpy
!pip install -U bitsandbytes

import os
import torch
import pandas as pd

import nltk
from konlpy.tag import Okt
import re

nltk.download('stopwords')
nltk.download('punkt')

from nltk.corpus import stopwords
english_stopwords = set(stopwords.words('english'))
okt = Okt()

def clean_text_columns(df, columns):
    if isinstance(columns, str):
        columns = [columns]

    for col in columns:
        # 전처리: 숫자, 공백, 특수문자 제거 강화
        df[col + '_cleaned'] = (
            df[col]
            .astype(str)
            .str.replace(r"\d+", "", regex=True)  # 숫자 제거
            .str.replace("\t|\n", " ")  # 탭, 줄바꿈 제거
            .str.replace(r" {2,}", " ", regex=True)  # 여러 공백 제거
            .str.replace(r"[\*\-,|!\?\"\'\(\)\[\]{}<>]", "", regex=True)  # 특수문자 제거 (확장)
            .str.strip()  # 앞뒤 공백 제거
        )

        # 불필요한 단어 제거: 한글과 영어 처리 강화
        def remove_unnecessary_words(text):
            if re.search('[가-힣]', text):  # 한국어가 포함된 경우
                tagged = okt.pos(text)  # 품사 태깅
                # 조사(Josa), 접속사(Conjunction), 형용사(Adjective), 부사(Adverb) 제거
                cleaned_words = [
                    word for word, tag in tagged
                    if tag not in ['Josa', 'Conjunction', 'Adjective', 'Adverb']
                ]
                return ' '.join(cleaned_words)
            else:  # 영어인 경우
                words = nltk.word_tokenize(text)  # 단어 토큰화
                # 영어에서 불용어와 짧은 단어 제거 (길이가 2 이하인 단어는 불필요할 수 있음)
                cleaned_words = [
                    word for word in words
                    if word.lower() not in english_stopwords and len(word) > 2
                ]
                return ' '.join(cleaned_words)

        # 'example_cleaned' 열에 대해 전처리 적용
        df[col + '_cleaned'] = df[col + '_cleaned'].apply(remove_unnecessary_words)

        # 빈값을 빈 문자열로 채움
        df[col + '_cleaned'].fillna("", inplace=True)

    return df

df = pd.read_csv("/content/sorted.csv")
df = clean_text_columns(df, 'example')

from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF 벡터화
vectorizer = TfidfVectorizer(max_features=10, stop_words='english')
tfidf_matrix = vectorizer.fit_transform(df['example_cleaned'])

# 각 문서에 대해 핵심어 추출
df['keywords'] = [list(vectorizer.get_feature_names_out()[tfidf_matrix[i].nonzero()[1]]) for i in range(tfidf_matrix.shape[0])]

!pip install rake-nltk

from rake_nltk import Rake

# RAKE 객체 생성
r = Rake()

# 각 문서에 대해 핵심어 추출
def extract_keywords_rake(text):
    r.extract_keywords_from_text(text)
    return r.get_ranked_phrases()  # 중요도 순으로 핵심어 리스트 반환

df['keywords_1'] = df['example_cleaned'].apply(extract_keywords_rake)

pip install keybert

from keybert import KeyBERT

# KeyBERT 객체 생성
kw_model = KeyBERT()

# 각 문서에 대해 핵심어 추출
df['keywords_2'] = df['example_cleaned'].apply(lambda x: kw_model.extract_keywords(x, top_n=5))

# from gensim.summarization import keywords

# # TextRank를 이용한 핵심어 추출
# def extract_keywords_textrank(text):
#     return keywords(text, words=5, lemmatize=True).split('\n')

# df['keywords_2'] = df['example_cleaned'].apply(extract_keywords_textrank)

df

df = df.drop(['example', 'example_cleaned', 'keywords'], axis = 1)

df

df.to_csv('sorted_cleaned.csv', index=False)

