# -*- coding: utf-8 -*-
"""Solar using Finetuning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Y_wUv52iXdI-2FVn-iDrh9NTS8V3iIN
"""

!pip install datasets --quiet
!pip install peft --quiet
!pip install bitsandbytes --quiet
!pip uninstall bitsandbytes -y --quiet
!pip install -U bitsandbytes --quiet
!pip install streamlit --quiet

import os
import torch
import pandas as pd
import transformers
from datasets import Dataset
from transformers import (
    AutoModelForQuestionAnswering,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    pipeline
)
from peft import (
    LoraConfig,
    prepare_model_for_kbit_training,
    get_peft_model,
    get_peft_model_state_dict,
    set_peft_model_state_dict,
    TaskType,
    PeftModel,
    PeftConfig  # 여기에 PeftConfig를 추가합니다.
)

# 데이터셋 로드
file_path = "./sorted.csv"
df = pd.read_csv(file_path, encoding='utf-8')

# 텍스트 전처리 함수 정의
def clean_text_columns(df, columns):
    if isinstance(columns, str):
        columns = [columns]

    for col in columns:
        df[col + '_cleaned'] = (
            df[col]
            .astype(str)
            .str.replace("\t|\n", " ")
            .str.replace(r" {2,}", " ", regex=True)
            .str.replace(r"[\*\-,|]", "", regex=True)
            .str.strip()
        )
        df[col + '_cleaned'].fillna("", inplace=True)

    return df

# example 열에 대해 전처리 수행
df = clean_text_columns(df, 'example')

dataset = Dataset.from_pandas(df)

from datasets import DatasetDict

# 데이터셋 분할 (train, eval)
train_test_split = dataset.train_test_split(test_size=0.1)  # 10%를 평가 데이터셋으로 사용
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']

# 모델과 토크나이저 정의
model_name = "bert-large-uncased-whole-word-masking-finetuned-squad"
model = AutoModelForQuestionAnswering.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Colab의 L4 GPU 사용
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 모델을 GPU로 이동
model.to(device)


def preprocess_function(examples):
    # ESRS와 GRI를 결합하여 입력 텍스트 생성
    inputs = [f"ESRS: {esrs} GRI: {gri}" for esrs, gri in zip(examples["ESRS"], examples["GRI"])]

    # 정답 텍스트를 레이블로 설정 (예: example_cleaned)
    answers = examples["example_cleaned"]

    # 토큰화
    model_inputs = tokenizer(
        inputs,
        max_length=512,  # 필요에 따라 수정
        truncation=True,
        padding="max_length",
        return_tensors="pt"
    )

    # 정답에 대한 start_positions과 end_positions 계산
    start_positions = []
    end_positions = []

    for i in range(len(inputs)):
        # 정답의 시작과 끝 인덱스 계산
        answer = answers[i]
        input_ids = model_inputs["input_ids"][i]
        answer_tokens = tokenizer(answer)["input_ids"]

        # 정답이 입력 텍스트에 존재하는지 확인
        start_index = input_ids.tolist().index(answer_tokens[1]) if answer_tokens[1] in input_ids.tolist() else -1
        end_index = start_index + len(answer_tokens) - 2 if start_index != -1 else -1

        start_positions.append(start_index)
        end_positions.append(end_index)

    # 레이블 추가
    model_inputs["start_positions"] = start_positions
    model_inputs["end_positions"] = end_positions

    return model_inputs

# train_dataset을 tokenized_dataset으로 변환
tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)
tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)


# Dataset 생성
dataset = Dataset.from_pandas(df)

from peft import get_peft_model, LoraConfig

# PEFT 설정을 정의
lora_config = LoraConfig(
    r=4,
    lora_alpha=8,
    lora_dropout=0.05,
    task_type=TaskType.CAUSAL_LM  # 이 부분은 사용하고 있는 작업 유형에 맞게 설정하세요
)

# 원본 모델에 PEFT 적용
peft_model = get_peft_model(model, lora_config)

from transformers import BitsAndBytesConfig

# 양자화 설정
nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # "non-float 4-bit" 양자화 방식을 의미
    bnb_4bit_use_double_quant=True,  # 두 단계 양자화
    bnb_4bit_compute_dtype=torch.bfloat16  # 모델의 연산에 사용할 데이터 타입
)

for param in model.parameters():
    param.requires_grad = True  # 모든 파라미터에 대해 gradient 계산 활성화

# # PEFT 모델 로드 후
# peft_model = PeftModel.from_pretrained(model, finetuned_model)

# # PEFT 모델의 파라미터도 활성화
# for param in peft_model.parameters():
#     param.requires_grad = True  # PEFT 모델의 모든 파라미터에 대해 gradient 계산 활성화

# 학습 인자 정의
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",  # 평가 전략
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Trainer 설정
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset,  # 평가 데이터셋 추가
)

# 훈련 시작
trainer.train()

trainer.save_model('/content/llmINNO')  # 모델 저장
from peft import PeftModel

# PEFT 모델 저장
peft_model.save_pretrained('/content/llmINNO')
import os

# # 파일이 저장된 경로
# model_path = '/content/llmINNO'

# # 해당 디렉토리에 어떤 파일이 있는지 확인
# files = os.listdir(model_path)
# print("Files in the model directory:", files)
# from peft import PeftConfig

# # PEFT 설정을 사용하여 모델 불러오기
# peft_config = PeftConfig.from_pretrained('/content/llmINNO')  # 저장한 경로로 수정
# from google.colab import drive
# drive.mount('/content/drive')

# # Google Drive에 모델 저장
# trainer.save_model('/content/drive/MyDrive/llmINNO')

# 모델 저장
finetuned_model = "/content/llmINNO"
trainer.save_model(finetuned_model)

# PEFT 모델 불러오기
peft_config = PeftConfig.from_pretrained(finetuned_model)
model = AutoModelForQuestionAnswering.from_pretrained(peft_config.base_model_name_or_path)
peft_model = PeftModel.from_pretrained(model, finetuned_model)

from transformers import AutoModelForQuestionAnswering, AutoTokenizer
import torch

# 모델과 토크나이저 로드
model_name = "bert-large-uncased-whole-word-masking-finetuned-squad"
model = AutoModelForQuestionAnswering.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Colab의 L4 GPU 사용
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 모델을 GPU로 이동
model.to(device)

# 입력 데이터 정의
input_text = "ESRS에 대해 알려줘"
context = "여기에 ESRS와 관련된 문서나 텍스트를 입력하세요."

# 입력 데이터 토큰화
inputs = tokenizer(input_text, context, return_tensors="pt").to(device)

# 모델 예측 수행
with torch.no_grad():
    outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits

# 결과 출력
print("Start logits:", start_logits)
print("End logits:", end_logits)

# Basemodel, tokenizer 로드
# 양자화를 사용하지 않는다면 quantization_config는 제거해야 함
model = AutoModelForQuestionAnswering.from_pretrained(
    peft_config.base_model_name_or_path,
    #quantization_config=nf4_config,  # 양자화를 지원하지 않으면 이 부분을 제거
    torch_dtype=torch.bfloat16
)

# 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained(
    peft_config.base_model_name_or_path
)

# 모델을 수동으로 GPU로 이동 (Colab에서 L4 GPU 사용)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# QLoRA 모델 로드
peft_model = PeftModel.from_pretrained(model, finetuned_model, torch_dtype=torch.bfloat16)

# QLoRA 가중치를 베이스 모델에 병합
merged_model = peft_model.merge_and_unload()

# 병합 후에도 GPU로 모델 이동
merged_model.to(device)

import torch
from transformers import pipeline

# GPU가 가능할 경우, 장치를 GPU로 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 모델을 float16으로 로드 (GPU에서 주로 사용하는 형식)
model = model.to(device, torch.float16)

# 입력 데이터
input_text = "ESRS에 대해 알려줘"
context = "여기에 ESRS와 관련된 문서나 텍스트를 입력하세요."

# 질문-답변 파이프라인 설정 (GPU가 있으면 device 0으로 설정)
pipe = pipeline(
    "question-answering",
    model=model,
    tokenizer=tokenizer,
    device=0  # GPU를 사용할 경우: device=0 (첫 번째 GPU)
)

# 질문-답변 수행, max_answer_len으로 답변 길이 제한 설정
result = pipe({
    "question": input_text,
    "context": context
}, max_answer_len=200)  # max_answer_len을 통해 최대 답변 길이 지정

print("Answer:", result['answer'])

!pip install openai --quiet

model.save_pretrained("/content/llmINNO")
tokenizer.save_pretrained("/content/llmINNO")

from openai import OpenAI  # openai==1.2.0

# Initialize the API client
client = OpenAI(
    api_key="up_80hKmHMBwNVzccFIZ1XEdM3A43PA0",
    base_url="https://api.upstage.ai/v1/solar"
)

# Initialize the conversation history
conversation_history = [
    {
        "role": "system",
        "content": "You are a helpful assistant."
    }
]

def send_message(user_message):
    # Add user message to conversation history
    conversation_history.append({
        "role": "user",
        "content": user_message
    })

    # Create a chat completion request
    stream = client.chat.completions.create(
        model="solar-pro",
        messages=conversation_history,
        stream=True,
    )

    # Stream the response
    response_content = ""
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            response_content += chunk.choices[0].delta.content

    # Add assistant's response to the conversation history
    conversation_history.append({
        "role": "assistant",
        "content": response_content
    })

    return response_content

# Example of sending a message
user_input = "Hi, how are you?"
response = send_message(user_input)
print("Assistant:", response)

# You can continue the conversation by calling send_message with new user input

from transformers import AutoModelForQuestionAnswering, AutoTokenizer
from huggingface_hub import login

# Hugging Face Hub에 로그인
login(token="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")

# 모델과 토크나이저 불러오기
model = AutoModelForQuestionAnswering.from_pretrained("/content/llmINNO")
tokenizer = AutoTokenizer.from_pretrained("/content/llmINNO")

# 모델 업로드
model.push_to_hub("finetuned_model")
tokenizer.push_to_hub("llminno_tokenizer")

from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline
from openai import OpenAI

# OpenAI API 클라이언트 생성
client = OpenAI(
    api_key="up_80hKmHMBwNVzccFIZ1XEdM3A43PA0",
    base_url="https://api.upstage.ai/v1/solar"
)
from huggingface_hub import login
from transformers import AutoModelForQuestionAnswering, AutoTokenizer

# Hugging Face Hub에 로그인
login(token="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")

# 모델과 토크나이저 불러오기
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = AutoModelForQuestionAnswering.from_pretrained(
    "easyoon/finetuned_model",
    torch_dtype=torch.float16
).to(device)  # 명시적으로 GPU로 이동

# 질문-답변 파이프라인 설정
qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer)

def get_response_from_api(question):
    stream = client.chat.completions.create(
        model="solar-pro",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": question
            }
        ],
        stream=True,
    )

    response = ""
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            response += chunk.choices[0].delta.content
    return response

# 질문-답변 수행 및 Solar API 연결
def answer_question(question, context):
    # Hugging Face 모델로 질문에 대한 답변 얻기
    hf_result = qa_pipeline({
        "question": question,
        "context": context
    })

    # 모델의 답변이 부족할 경우 Solar API를 사용하여 응답을 받음
    if len(hf_result['answer']) < 20:  # 답변 길이에 따라 조건 설정 가능
        api_response = get_response_from_api(question)
        return api_response

    return hf_result['answer']

# 예시 입력
input_question = "ESG의 ESRS 지표에 대해 알려줘"
context = "여기에 ESRS와 관련된 문서나 텍스트를 입력하세요."

# 질문에 대한 답변 출력
final_answer = answer_question(input_question, context)
print("Final Answer:", final_answer)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile llm_model.py
# import pandas as pd
# from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer
# import torch
# 
# # 글로벌 변수로 문서 컨텍스트를 캐시
# document_context = None
# 
# # 문서 로드 및 캐시 함수
# def load_document(file_path):
#     global document_context
#     if document_context is None:
#         try:
#             document_context = pd.read_csv(file_path)  # 캐시된 데이터프레임
#         except Exception as e:
#             document_context = None
#             raise ValueError(f"파일을 불러오는 데 문제가 발생했습니다: {e}")
# 
# # 질문에서 ESRS 코드를 추출하여 관련 문맥 찾기
# def find_relevant_esrs(question, df):
#     # ESRS 코드를 추출하여 해당 정보 찾기
#     esrs_code = None
#     for code in df['ESRS']:
#         if code in question:
#             esrs_code = code
#             break
# 
#     if esrs_code:
#         # 해당 ESRS 코드에 대한 설명을 모두 가져와서 문맥으로 제공
#         row = df[df['ESRS'] == esrs_code]
#         if not row.empty:
#             return row['example'].values[0]
# 
#     return "관련된 ESRS 정보를 찾을 수 없습니다."
# 
# # 모델 및 토크나이저 로드 (한 번만 로드하여 성능 최적화)
# model_name = "/content/llmINNO"
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)
# 
# # 질문에 답변 생성하는 함수
# def answer_question_with_context(question, file_path):
#     # 문서 컨텍스트 로드 (캐싱된 경우 다시 읽지 않음)
#     load_document(file_path)
#     if document_context is None:
#         return "문서를 불러오는 데 문제가 발생했습니다."
# 
#     # 질문과 관련된 ESRS 항목 찾기
#     relevant_context = find_relevant_esrs(question, document_context)
# 
#     # 문맥이 충분히 추출되지 않았을 경우 대비
#     if relevant_context == "관련된 ESRS 정보를 찾을 수 없습니다.":
#         return relevant_context
# 
#     # 문맥의 길이를 제한하지 않고 제공
#     try:
#         result = qa_pipeline({
#             "question": question,
#             "context": relevant_context
#         })
#         return result['answer']
#     except Exception as e:
#         return f"답변 생성 오류: {e}"
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import streamlit as st
# from llm_model import answer_question_with_context  # llm_model.py에서 answer_question_with_context 함수 불러오기
# 
# # Streamlit 페이지 구성 설정
# st.set_page_config(
#     page_title="Fine-tuned Data 기반 챗봇",
#     page_icon="🤖",
#     layout="centered",
#     initial_sidebar_state="auto",
# )
# 
# # 앱 제목 설정
# st.title("🤖 Fine-tuned Data 기반 LLM Chatbot")
# st.write("안녕하세요! 질문이 있으시면 말씀해 주세요.")
# 
# # 디렉토리에 이미 저장된 파일 경로
# file_path = "/content/sorted.csv"  # Colab 환경에서의 파일 경로
# 
# # 사용자로부터 질문을 입력 받음
# user_input = st.text_input("당신의 질문을 입력하세요:")
# 
# # 사용자 질문에 대한 답변 생성
# if user_input:
#     with st.spinner('답변 생성 중...'):
#         response = answer_question_with_context(user_input, file_path)
#         st.write("챗봇: ", response)
#

!npm install localtunnel --quiet

import urllib
print("Password/Endpoint IP:", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip("\n"))

!streamlit run /content/app.py &>/content/logs.txt &

os.system("pkill -f 'lt --port'")
!npx localtunnel --port 8501