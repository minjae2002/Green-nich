# -*- coding: utf-8 -*-
"""Solar using Finetuning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Y_wUv52iXdI-2FVn-iDrh9NTS8V3iIN
"""

!pip install datasets --quiet
!pip install peft --quiet
!pip install bitsandbytes --quiet
!pip uninstall bitsandbytes -y --quiet
!pip install -U bitsandbytes --quiet
!pip install streamlit --quiet

import os
import torch
import pandas as pd
import transformers
from datasets import Dataset
from transformers import (
    AutoModelForQuestionAnswering,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    pipeline
)
from peft import (
    LoraConfig,
    prepare_model_for_kbit_training,
    get_peft_model,
    get_peft_model_state_dict,
    set_peft_model_state_dict,
    TaskType,
    PeftModel,
    PeftConfig  # ì—¬ê¸°ì— PeftConfigë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
)

# ë°ì´í„°ì…‹ ë¡œë“œ
file_path = "./sorted.csv"
df = pd.read_csv(file_path, encoding='utf-8')

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def clean_text_columns(df, columns):
    if isinstance(columns, str):
        columns = [columns]

    for col in columns:
        df[col + '_cleaned'] = (
            df[col]
            .astype(str)
            .str.replace("\t|\n", " ")
            .str.replace(r" {2,}", " ", regex=True)
            .str.replace(r"[\*\-,|]", "", regex=True)
            .str.strip()
        )
        df[col + '_cleaned'].fillna("", inplace=True)

    return df

# example ì—´ì— ëŒ€í•´ ì „ì²˜ë¦¬ ìˆ˜í–‰
df = clean_text_columns(df, 'example')

dataset = Dataset.from_pandas(df)

from datasets import DatasetDict

# ë°ì´í„°ì…‹ ë¶„í•  (train, eval)
train_test_split = dataset.train_test_split(test_size=0.1)  # 10%ë¥¼ í‰ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì •ì˜
model_name = "bert-large-uncased-whole-word-masking-finetuned-squad"
model = AutoModelForQuestionAnswering.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Colabì˜ L4 GPU ì‚¬ìš©
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ëª¨ë¸ì„ GPUë¡œ ì´ë™
model.to(device)


def preprocess_function(examples):
    # ESRSì™€ GRIë¥¼ ê²°í•©í•˜ì—¬ ì…ë ¥ í…ìŠ¤íŠ¸ ìƒì„±
    inputs = [f"ESRS: {esrs} GRI: {gri}" for esrs, gri in zip(examples["ESRS"], examples["GRI"])]

    # ì •ë‹µ í…ìŠ¤íŠ¸ë¥¼ ë ˆì´ë¸”ë¡œ ì„¤ì • (ì˜ˆ: example_cleaned)
    answers = examples["example_cleaned"]

    # í† í°í™”
    model_inputs = tokenizer(
        inputs,
        max_length=512,  # í•„ìš”ì— ë”°ë¼ ìˆ˜ì •
        truncation=True,
        padding="max_length",
        return_tensors="pt"
    )

    # ì •ë‹µì— ëŒ€í•œ start_positionsê³¼ end_positions ê³„ì‚°
    start_positions = []
    end_positions = []

    for i in range(len(inputs)):
        # ì •ë‹µì˜ ì‹œì‘ê³¼ ë ì¸ë±ìŠ¤ ê³„ì‚°
        answer = answers[i]
        input_ids = model_inputs["input_ids"][i]
        answer_tokens = tokenizer(answer)["input_ids"]

        # ì •ë‹µì´ ì…ë ¥ í…ìŠ¤íŠ¸ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        start_index = input_ids.tolist().index(answer_tokens[1]) if answer_tokens[1] in input_ids.tolist() else -1
        end_index = start_index + len(answer_tokens) - 2 if start_index != -1 else -1

        start_positions.append(start_index)
        end_positions.append(end_index)

    # ë ˆì´ë¸” ì¶”ê°€
    model_inputs["start_positions"] = start_positions
    model_inputs["end_positions"] = end_positions

    return model_inputs

# train_datasetì„ tokenized_datasetìœ¼ë¡œ ë³€í™˜
tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)
tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)


# Dataset ìƒì„±
dataset = Dataset.from_pandas(df)

from peft import get_peft_model, LoraConfig

# PEFT ì„¤ì •ì„ ì •ì˜
lora_config = LoraConfig(
    r=4,
    lora_alpha=8,
    lora_dropout=0.05,
    task_type=TaskType.CAUSAL_LM  # ì´ ë¶€ë¶„ì€ ì‚¬ìš©í•˜ê³  ìˆëŠ” ì‘ì—… ìœ í˜•ì— ë§ê²Œ ì„¤ì •í•˜ì„¸ìš”
)

# ì›ë³¸ ëª¨ë¸ì— PEFT ì ìš©
peft_model = get_peft_model(model, lora_config)

from transformers import BitsAndBytesConfig

# ì–‘ìí™” ì„¤ì •
nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # "non-float 4-bit" ì–‘ìí™” ë°©ì‹ì„ ì˜ë¯¸
    bnb_4bit_use_double_quant=True,  # ë‘ ë‹¨ê³„ ì–‘ìí™”
    bnb_4bit_compute_dtype=torch.bfloat16  # ëª¨ë¸ì˜ ì—°ì‚°ì— ì‚¬ìš©í•  ë°ì´í„° íƒ€ì…
)

for param in model.parameters():
    param.requires_grad = True  # ëª¨ë“  íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ gradient ê³„ì‚° í™œì„±í™”

# # PEFT ëª¨ë¸ ë¡œë“œ í›„
# peft_model = PeftModel.from_pretrained(model, finetuned_model)

# # PEFT ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë„ í™œì„±í™”
# for param in peft_model.parameters():
#     param.requires_grad = True  # PEFT ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ gradient ê³„ì‚° í™œì„±í™”

# í•™ìŠµ ì¸ì ì •ì˜
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",  # í‰ê°€ ì „ëµ
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Trainer ì„¤ì •
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset,  # í‰ê°€ ë°ì´í„°ì…‹ ì¶”ê°€
)

# í›ˆë ¨ ì‹œì‘
trainer.train()

trainer.save_model('/content/llmINNO')  # ëª¨ë¸ ì €ì¥
from peft import PeftModel

# PEFT ëª¨ë¸ ì €ì¥
peft_model.save_pretrained('/content/llmINNO')
import os

# # íŒŒì¼ì´ ì €ì¥ëœ ê²½ë¡œ
# model_path = '/content/llmINNO'

# # í•´ë‹¹ ë””ë ‰í† ë¦¬ì— ì–´ë–¤ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
# files = os.listdir(model_path)
# print("Files in the model directory:", files)
# from peft import PeftConfig

# # PEFT ì„¤ì •ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
# peft_config = PeftConfig.from_pretrained('/content/llmINNO')  # ì €ì¥í•œ ê²½ë¡œë¡œ ìˆ˜ì •
# from google.colab import drive
# drive.mount('/content/drive')

# # Google Driveì— ëª¨ë¸ ì €ì¥
# trainer.save_model('/content/drive/MyDrive/llmINNO')

# ëª¨ë¸ ì €ì¥
finetuned_model = "/content/llmINNO"
trainer.save_model(finetuned_model)

# PEFT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
peft_config = PeftConfig.from_pretrained(finetuned_model)
model = AutoModelForQuestionAnswering.from_pretrained(peft_config.base_model_name_or_path)
peft_model = PeftModel.from_pretrained(model, finetuned_model)

from transformers import AutoModelForQuestionAnswering, AutoTokenizer
import torch

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "bert-large-uncased-whole-word-masking-finetuned-squad"
model = AutoModelForQuestionAnswering.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Colabì˜ L4 GPU ì‚¬ìš©
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ëª¨ë¸ì„ GPUë¡œ ì´ë™
model.to(device)

# ì…ë ¥ ë°ì´í„° ì •ì˜
input_text = "ESRSì— ëŒ€í•´ ì•Œë ¤ì¤˜"
context = "ì—¬ê¸°ì— ESRSì™€ ê´€ë ¨ëœ ë¬¸ì„œë‚˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”."

# ì…ë ¥ ë°ì´í„° í† í°í™”
inputs = tokenizer(input_text, context, return_tensors="pt").to(device)

# ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰
with torch.no_grad():
    outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits

# ê²°ê³¼ ì¶œë ¥
print("Start logits:", start_logits)
print("End logits:", end_logits)

# Basemodel, tokenizer ë¡œë“œ
# ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ quantization_configëŠ” ì œê±°í•´ì•¼ í•¨
model = AutoModelForQuestionAnswering.from_pretrained(
    peft_config.base_model_name_or_path,
    #quantization_config=nf4_config,  # ì–‘ìí™”ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë©´ ì´ ë¶€ë¶„ì„ ì œê±°
    torch_dtype=torch.bfloat16
)

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(
    peft_config.base_model_name_or_path
)

# ëª¨ë¸ì„ ìˆ˜ë™ìœ¼ë¡œ GPUë¡œ ì´ë™ (Colabì—ì„œ L4 GPU ì‚¬ìš©)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# QLoRA ëª¨ë¸ ë¡œë“œ
peft_model = PeftModel.from_pretrained(model, finetuned_model, torch_dtype=torch.bfloat16)

# QLoRA ê°€ì¤‘ì¹˜ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©
merged_model = peft_model.merge_and_unload()

# ë³‘í•© í›„ì—ë„ GPUë¡œ ëª¨ë¸ ì´ë™
merged_model.to(device)

import torch
from transformers import pipeline

# GPUê°€ ê°€ëŠ¥í•  ê²½ìš°, ì¥ì¹˜ë¥¼ GPUë¡œ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ëª¨ë¸ì„ float16ìœ¼ë¡œ ë¡œë“œ (GPUì—ì„œ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” í˜•ì‹)
model = model.to(device, torch.float16)

# ì…ë ¥ ë°ì´í„°
input_text = "ESRSì— ëŒ€í•´ ì•Œë ¤ì¤˜"
context = "ì—¬ê¸°ì— ESRSì™€ ê´€ë ¨ëœ ë¬¸ì„œë‚˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”."

# ì§ˆë¬¸-ë‹µë³€ íŒŒì´í”„ë¼ì¸ ì„¤ì • (GPUê°€ ìˆìœ¼ë©´ device 0ìœ¼ë¡œ ì„¤ì •)
pipe = pipeline(
    "question-answering",
    model=model,
    tokenizer=tokenizer,
    device=0  # GPUë¥¼ ì‚¬ìš©í•  ê²½ìš°: device=0 (ì²« ë²ˆì§¸ GPU)
)

# ì§ˆë¬¸-ë‹µë³€ ìˆ˜í–‰, max_answer_lenìœ¼ë¡œ ë‹µë³€ ê¸¸ì´ ì œí•œ ì„¤ì •
result = pipe({
    "question": input_text,
    "context": context
}, max_answer_len=200)  # max_answer_lenì„ í†µí•´ ìµœëŒ€ ë‹µë³€ ê¸¸ì´ ì§€ì •

print("Answer:", result['answer'])

!pip install openai --quiet

model.save_pretrained("/content/llmINNO")
tokenizer.save_pretrained("/content/llmINNO")

from openai import OpenAI  # openai==1.2.0

# Initialize the API client
client = OpenAI(
    api_key="up_80hKmHMBwNVzccFIZ1XEdM3A43PA0",
    base_url="https://api.upstage.ai/v1/solar"
)

# Initialize the conversation history
conversation_history = [
    {
        "role": "system",
        "content": "You are a helpful assistant."
    }
]

def send_message(user_message):
    # Add user message to conversation history
    conversation_history.append({
        "role": "user",
        "content": user_message
    })

    # Create a chat completion request
    stream = client.chat.completions.create(
        model="solar-pro",
        messages=conversation_history,
        stream=True,
    )

    # Stream the response
    response_content = ""
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            response_content += chunk.choices[0].delta.content

    # Add assistant's response to the conversation history
    conversation_history.append({
        "role": "assistant",
        "content": response_content
    })

    return response_content

# Example of sending a message
user_input = "Hi, how are you?"
response = send_message(user_input)
print("Assistant:", response)

# You can continue the conversation by calling send_message with new user input

from transformers import AutoModelForQuestionAnswering, AutoTokenizer
from huggingface_hub import login

# Hugging Face Hubì— ë¡œê·¸ì¸
login(token="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")

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
model = AutoModelForQuestionAnswering.from_pretrained("/content/llmINNO")
tokenizer = AutoTokenizer.from_pretrained("/content/llmINNO")

# ëª¨ë¸ ì—…ë¡œë“œ
model.push_to_hub("finetuned_model")
tokenizer.push_to_hub("llminno_tokenizer")

from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline
from openai import OpenAI

# OpenAI API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = OpenAI(
    api_key="up_80hKmHMBwNVzccFIZ1XEdM3A43PA0",
    base_url="https://api.upstage.ai/v1/solar"
)
from huggingface_hub import login
from transformers import AutoModelForQuestionAnswering, AutoTokenizer

# Hugging Face Hubì— ë¡œê·¸ì¸
login(token="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")

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = AutoModelForQuestionAnswering.from_pretrained(
    "easyoon/finetuned_model",
    torch_dtype=torch.float16
).to(device)  # ëª…ì‹œì ìœ¼ë¡œ GPUë¡œ ì´ë™

# ì§ˆë¬¸-ë‹µë³€ íŒŒì´í”„ë¼ì¸ ì„¤ì •
qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer)

def get_response_from_api(question):
    stream = client.chat.completions.create(
        model="solar-pro",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": question
            }
        ],
        stream=True,
    )

    response = ""
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            response += chunk.choices[0].delta.content
    return response

# ì§ˆë¬¸-ë‹µë³€ ìˆ˜í–‰ ë° Solar API ì—°ê²°
def answer_question(question, context):
    # Hugging Face ëª¨ë¸ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì–»ê¸°
    hf_result = qa_pipeline({
        "question": question,
        "context": context
    })

    # ëª¨ë¸ì˜ ë‹µë³€ì´ ë¶€ì¡±í•  ê²½ìš° Solar APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ë°›ìŒ
    if len(hf_result['answer']) < 20:  # ë‹µë³€ ê¸¸ì´ì— ë”°ë¼ ì¡°ê±´ ì„¤ì • ê°€ëŠ¥
        api_response = get_response_from_api(question)
        return api_response

    return hf_result['answer']

# ì˜ˆì‹œ ì…ë ¥
input_question = "ESGì˜ ESRS ì§€í‘œì— ëŒ€í•´ ì•Œë ¤ì¤˜"
context = "ì—¬ê¸°ì— ESRSì™€ ê´€ë ¨ëœ ë¬¸ì„œë‚˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”."

# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì¶œë ¥
final_answer = answer_question(input_question, context)
print("Final Answer:", final_answer)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile llm_model.py
# import pandas as pd
# from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer
# import torch
# 
# # ê¸€ë¡œë²Œ ë³€ìˆ˜ë¡œ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìºì‹œ
# document_context = None
# 
# # ë¬¸ì„œ ë¡œë“œ ë° ìºì‹œ í•¨ìˆ˜
# def load_document(file_path):
#     global document_context
#     if document_context is None:
#         try:
#             document_context = pd.read_csv(file_path)  # ìºì‹œëœ ë°ì´í„°í”„ë ˆì„
#         except Exception as e:
#             document_context = None
#             raise ValueError(f"íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
# 
# # ì§ˆë¬¸ì—ì„œ ESRS ì½”ë“œë¥¼ ì¶”ì¶œí•˜ì—¬ ê´€ë ¨ ë¬¸ë§¥ ì°¾ê¸°
# def find_relevant_esrs(question, df):
#     # ESRS ì½”ë“œë¥¼ ì¶”ì¶œí•˜ì—¬ í•´ë‹¹ ì •ë³´ ì°¾ê¸°
#     esrs_code = None
#     for code in df['ESRS']:
#         if code in question:
#             esrs_code = code
#             break
# 
#     if esrs_code:
#         # í•´ë‹¹ ESRS ì½”ë“œì— ëŒ€í•œ ì„¤ëª…ì„ ëª¨ë‘ ê°€ì ¸ì™€ì„œ ë¬¸ë§¥ìœ¼ë¡œ ì œê³µ
#         row = df[df['ESRS'] == esrs_code]
#         if not row.empty:
#             return row['example'].values[0]
# 
#     return "ê´€ë ¨ëœ ESRS ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
# 
# # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ ì„±ëŠ¥ ìµœì í™”)
# model_name = "/content/llmINNO"
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)
# 
# # ì§ˆë¬¸ì— ë‹µë³€ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
# def answer_question_with_context(question, file_path):
#     # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ (ìºì‹±ëœ ê²½ìš° ë‹¤ì‹œ ì½ì§€ ì•ŠìŒ)
#     load_document(file_path)
#     if document_context is None:
#         return "ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
# 
#     # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ESRS í•­ëª© ì°¾ê¸°
#     relevant_context = find_relevant_esrs(question, document_context)
# 
#     # ë¬¸ë§¥ì´ ì¶©ë¶„íˆ ì¶”ì¶œë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ëŒ€ë¹„
#     if relevant_context == "ê´€ë ¨ëœ ESRS ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.":
#         return relevant_context
# 
#     # ë¬¸ë§¥ì˜ ê¸¸ì´ë¥¼ ì œí•œí•˜ì§€ ì•Šê³  ì œê³µ
#     try:
#         result = qa_pipeline({
#             "question": question,
#             "context": relevant_context
#         })
#         return result['answer']
#     except Exception as e:
#         return f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}"
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import streamlit as st
# from llm_model import answer_question_with_context  # llm_model.pyì—ì„œ answer_question_with_context í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
# 
# # Streamlit í˜ì´ì§€ êµ¬ì„± ì„¤ì •
# st.set_page_config(
#     page_title="Fine-tuned Data ê¸°ë°˜ ì±—ë´‡",
#     page_icon="ğŸ¤–",
#     layout="centered",
#     initial_sidebar_state="auto",
# )
# 
# # ì•± ì œëª© ì„¤ì •
# st.title("ğŸ¤– Fine-tuned Data ê¸°ë°˜ LLM Chatbot")
# st.write("ì•ˆë…•í•˜ì„¸ìš”! ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”.")
# 
# # ë””ë ‰í† ë¦¬ì— ì´ë¯¸ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
# file_path = "/content/sorted.csv"  # Colab í™˜ê²½ì—ì„œì˜ íŒŒì¼ ê²½ë¡œ
# 
# # ì‚¬ìš©ìë¡œë¶€í„° ì§ˆë¬¸ì„ ì…ë ¥ ë°›ìŒ
# user_input = st.text_input("ë‹¹ì‹ ì˜ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
# 
# # ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
# if user_input:
#     with st.spinner('ë‹µë³€ ìƒì„± ì¤‘...'):
#         response = answer_question_with_context(user_input, file_path)
#         st.write("ì±—ë´‡: ", response)
#

!npm install localtunnel --quiet

import urllib
print("Password/Endpoint IP:", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip("\n"))

!streamlit run /content/app.py &>/content/logs.txt &

os.system("pkill -f 'lt --port'")
!npx localtunnel --port 8501